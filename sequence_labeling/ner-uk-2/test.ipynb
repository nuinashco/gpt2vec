{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0c3231dcfc96>:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from tqdm.autonotebook import tqdm\n",
    "from gpt2vec.utils.other import set_seeds\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "set_seeds(seed=42)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dataset_name: str = \"Goader/ner-uk-2.0\"\n",
    "    \n",
    "    pretrained: str = \"benjamin/roberta-large-wechsel-ukrainian\"\n",
    "    # pretrained: str = \"microsoft/mdeberta-v3-base\"\n",
    "    max_length: int = 1024\n",
    "    merge_subwords: bool = True\n",
    "    \n",
    "    # wandb_init_args = {\n",
    "    #     'project': \"sl-ner-uk-2.0\",\n",
    "    #     'entity': \"havlytskyi-thesis\",\n",
    "    #     'name': \"benjamin--word-level\"\n",
    "    # }\n",
    "    wandb_init_args = {\n",
    "        'project': \"thesis-sequence-labeling\",\n",
    "        'entity': \"ivan-havlytskyiz\",\n",
    "        'name': \"benjamin--word-level\"\n",
    "    }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./checkpoints/{config.wandb_init_args[\"name\"]}',\n",
    "    logging_dir=f'./logs/{config.wandb_init_args[\"name\"]}',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    #bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_torch',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    # eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    # save_steps=100,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.pretrained)\n",
    "tokenizer.add_prefix_space=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NerUKDataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m NerUKDataset(\n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      5\u001b[0m     splits\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdataset_name,\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils.data import NerUKDataset\n",
    "\n",
    "dataset = NerUKDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    splits=('train', 'validation', 'test'),\n",
    "    dataset_name=config.dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    config.pretrained,\n",
    "    num_labels=len(dataset.label_list),\n",
    "    id2label=dataset.id2label,\n",
    "    label2id=dataset.label2id,\n",
    "    )\n",
    "\n",
    "base_model = AutoModel.from_pretrained(config.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word_model import ModelForWordTask\n",
    "\n",
    "model = ModelForWordTask(\n",
    "    model=base_model,\n",
    "    merge_subwords=True,\n",
    "    config=model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from utils.metric import NerMetrics\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=dataset.train,\n",
    "    eval_dataset=dataset.val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=NerMetrics(label2id=dataset.label2id).compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(**config.wandb_init_args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metric import score as char_f1\n",
    "from utils.utils import inference_aggregation\n",
    "\n",
    "FINETUNED_MODEL = 'checkpoints/mdeberta-v3-base/checkpoint-300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._load_from_checkpoint(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcf14f88a7d402f85192e0bcd5494a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce23fb232e14812bfe9215012448620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5634806362192857,\n",
       " 'recall': 0.7106647658808954,\n",
       " 'f1': 0.6285716430120762,\n",
       " 'thold': 0.16}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds = trainer.predict(ds_valid)\n",
    "valid_metrics = trainer.compute_metrics((valid_preds.predictions, valid_preds.label_ids))\n",
    "\n",
    "valid_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514db1fc9d6d47b5b08e878ebba14e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.23767676767676768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.utils import find_class_balance_threshold\n",
    "\n",
    "test_preds = trainer.predict(ds_test)\n",
    "test_probabilities = torch.softmax(torch.tensor(test_preds.predictions), dim=-1).cpu().numpy()\n",
    "\n",
    "test_distr_th = find_class_balance_threshold(\n",
    "    desired_positive_ratio=positive_class_balance,\n",
    "    probabilities=test_probabilities,\n",
    "    labels=test_preds.label_ids\n",
    "    )\n",
    "\n",
    "print(test_distr_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_th = valid_metrics['thold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probabilities = torch.softmax(torch.tensor(valid_preds.predictions), dim=-1).cpu().numpy()\n",
    "valid_results = inference_aggregation(\n",
    "    probabilities=valid_probabilities,\n",
    "    labels=valid_preds.label_ids,\n",
    "    offset_mappings=ds_valid['offset_mapping'],\n",
    "    thold=final_th\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6285716430120762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df_valid_gt = df[df.fold==4][['id', 'trigger_words']].reset_index(drop=True)\n",
    "df_valid = deepcopy(df_valid_gt)\n",
    "df_valid['trigger_words'] = valid_results\n",
    "\n",
    "cv_score = char_f1(df_valid_gt, df_valid, row_id_column_name='id')\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = inference_aggregation(\n",
    "    probabilities=test_probabilities,\n",
    "    labels=test_preds.label_ids,\n",
    "    offset_mappings=ds_test['offset_mapping'],\n",
    "    thold=final_th\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6047970079507895"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_gt = pd.read_csv(config.data_path + 'solution.csv')[['id', 'trigger_words']]\n",
    "df_test = deepcopy(df_test_gt)\n",
    "df_test['trigger_words'] = test_results\n",
    "\n",
    "test_score = char_f1(df_test_gt, df_test, row_id_column_name='id')\n",
    "test_score"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10931355,
     "sourceId": 89664,
     "sourceType": "competition"
    },
    {
     "datasetId": 6604871,
     "sourceId": 10664686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
