{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0c3231dcfc96>:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from tqdm.autonotebook import tqdm\n",
    "from gpt2vec.utils.other import set_seeds\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "set_seeds(seed=42)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dataset_name: str = \"Goader/ner-uk-2.0\"\n",
    "    \n",
    "    pretrained: str = \"microsoft/mdeberta-v3-base\"\n",
    "    # max_length: int = 1024\n",
    "    merge_subwords: bool = True\n",
    "    \n",
    "    wandb_init_args = {\n",
    "        'project': \"sl-ner-uk-2.0\",\n",
    "        'entity': \"havlytskyi-thesis\",\n",
    "        'name': \"mdeberta-v3-base--word-level\"\n",
    "    }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./checkpoints/{config.wandb_init_args[\"name\"]}',\n",
    "    logging_dir=f'./logs/{config.wandb_init_args[\"name\"]}',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    #bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_torch',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5d4d0e6abf48a2b5ddb0568f9cf18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split:   0%|          | 0/10980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674302f8a0954b47b9c46645ac16bbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf29027b528549a2afa4c1be04d1d05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test split:   0%|          | 0/5593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from word_utils.data import NerUKDataset\n",
    "\n",
    "dataset = NerUKDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    splits=('train', 'validation', 'test'),\n",
    "    dataset_name=config.dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained(\n",
    "    config.pretrained,\n",
    "    num_labels=len(dataset.label_list),\n",
    "    id2label=dataset.id2label,\n",
    "    label2id=dataset.label2id,\n",
    "    )\n",
    "\n",
    "base_model = AutoModel.from_pretrained(config.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_utils.word_model import ModelForWordTask\n",
    "\n",
    "model = ModelForWordTask(\n",
    "    model=base_model,\n",
    "    merge_subwords=True,\n",
    "    config=model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f18be1148c4f>:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from word_utils.metric import NerMetrics\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=dataset.train,\n",
    "    eval_dataset=dataset.val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=NerMetrics(label2id=dataset.label2id).compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-havlytskyi\u001b[0m (\u001b[33mivan-havlytskyiz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/GPT2Vec/sequence_labeling/ner-uk-2/wandb/run-20250513_095011-nxszr49e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/nxszr49e' target=\"_blank\">mdeberta-v3-base--word-level</a></strong> to <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0' target=\"_blank\">https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/nxszr49e' target=\"_blank\">https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/nxszr49e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1715' max='1715' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1715/1715 26:18, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.186618</td>\n",
       "      <td>0.674260</td>\n",
       "      <td>0.692668</td>\n",
       "      <td>0.683340</td>\n",
       "      <td>0.953512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.131532</td>\n",
       "      <td>0.695291</td>\n",
       "      <td>0.783151</td>\n",
       "      <td>0.736610</td>\n",
       "      <td>0.960871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.117681</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>0.811232</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0.965489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.130645</td>\n",
       "      <td>0.743281</td>\n",
       "      <td>0.819813</td>\n",
       "      <td>0.779674</td>\n",
       "      <td>0.962546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.110470</td>\n",
       "      <td>0.798808</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.969702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.112097</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.842434</td>\n",
       "      <td>0.820669</td>\n",
       "      <td>0.969905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.111615</td>\n",
       "      <td>0.789818</td>\n",
       "      <td>0.847114</td>\n",
       "      <td>0.817463</td>\n",
       "      <td>0.969549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.113741</td>\n",
       "      <td>0.787945</td>\n",
       "      <td>0.846334</td>\n",
       "      <td>0.816096</td>\n",
       "      <td>0.969702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-200)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-400)... Done. 3.6s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-600)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-800)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-1000)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-1200)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-1400)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-1600)... Done. 3.6s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--word-level/checkpoint-1715)... Done. 3.7s\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1715, training_loss=0.11426291197277715, metrics={'train_runtime': 1576.0014, 'train_samples_per_second': 34.835, 'train_steps_per_second': 1.088, 'total_flos': 2753882444671200.0, 'train_loss': 0.11426291197277715, 'epoch': 4.986899563318778})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(**config.wandb_init_args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_MODEL = f'checkpoints/{config.wandb_init_args[\"name\"]}/checkpoint-1200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._load_from_checkpoint(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8410812672176309,\n",
       " 'recall': 0.839058742700103,\n",
       " 'f1': 0.8400687876182287,\n",
       " 'accuracy': 0.9780628085712831}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = trainer.predict(dataset.test)\n",
    "test_metrics = trainer.compute_metrics((test_preds.predictions, test_preds.label_ids))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10931355,
     "sourceId": 89664,
     "sourceType": "competition"
    },
    {
     "datasetId": 6604871,
     "sourceId": 10664686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
