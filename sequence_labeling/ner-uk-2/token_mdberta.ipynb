{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-0c3231dcfc96>:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from tqdm.autonotebook import tqdm\n",
    "from gpt2vec.utils.other import set_seeds\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "set_seeds(seed=42)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dataset_name: str = \"Goader/ner-uk-2.0\"\n",
    "    \n",
    "    pretrained: str = \"microsoft/mdeberta-v3-base\"\n",
    "    # max_length: int = 1024\n",
    "    merge_subwords: bool = True\n",
    "    \n",
    "    wandb_init_args = {\n",
    "        'project': \"sl-ner-uk-2.0\",\n",
    "        'entity': \"havlytskyi-thesis\",\n",
    "        'name': \"mdeberta-v3-base--token-level\"\n",
    "    }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./checkpoints/{config.wandb_init_args[\"name\"]}',\n",
    "    logging_dir=f'./logs/{config.wandb_init_args[\"name\"]}',\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.0,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    bf16=False,\n",
    "    report_to=\"wandb\",\n",
    "    optim='adamw_torch',\n",
    "    eval_strategy='steps',\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=10,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    dataloader_drop_last=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06059dc3f06406e9aee80a31a9d71f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split:   0%|          | 0/10980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5c51c92e844fa0a1cc16ea6e9f8ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/1206 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2984df8b26bc4a44b7b52253b974fb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test split:   0%|          | 0/5593 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from token_utils.data import NerUKDataset\n",
    "\n",
    "dataset = NerUKDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    splits=('train', 'validation', 'test'),\n",
    "    dataset_name=config.dataset_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    config.pretrained,\n",
    "    id2label=dataset.id2label,\n",
    "    label2id=dataset.label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0 bad sequences\n",
      "validation 0 bad sequences\n",
      "test 0 bad sequences\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(dataset.label_list)\n",
    "\n",
    "def flag_bad(example):\n",
    "    return {\"bad\": any(l >= num_labels for l in example[\"labels\"]\n",
    "                       if l != -100)}\n",
    "\n",
    "for split in (\"train\", \"validation\", \"test\"):\n",
    "    bad_rows = dataset.aligned[split].filter(lambda example: any(l >= num_labels for l in example[\"labels\"] if l != -100))\n",
    "    print(split, bad_rows.num_rows, \"bad sequences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-b7b5b84b43d2>:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "from token_utils.metric import NerMetrics\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=dataset.train,\n",
    "    eval_dataset=dataset.val,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=NerMetrics(id2label=dataset.id2label).compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivan-havlytskyi\u001b[0m (\u001b[33mivan-havlytskyiz\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/GPT2Vec/sequence_labeling/ner-uk-2/wandb/run-20250513_114452-bs9ocypu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/bs9ocypu' target=\"_blank\">mdeberta-v3-base--token-level</a></strong> to <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0' target=\"_blank\">https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/bs9ocypu' target=\"_blank\">https://wandb.ai/havlytskyi-thesis/sl-ner-uk-2.0/runs/bs9ocypu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1715' max='1715' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1715/1715 05:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.189603</td>\n",
       "      <td>0.556231</td>\n",
       "      <td>0.575020</td>\n",
       "      <td>0.565469</td>\n",
       "      <td>0.942750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.147440</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.721131</td>\n",
       "      <td>0.670073</td>\n",
       "      <td>0.957099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.121462</td>\n",
       "      <td>0.676230</td>\n",
       "      <td>0.777690</td>\n",
       "      <td>0.723420</td>\n",
       "      <td>0.968453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.132209</td>\n",
       "      <td>0.687292</td>\n",
       "      <td>0.811469</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>0.964812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.114276</td>\n",
       "      <td>0.746077</td>\n",
       "      <td>0.821681</td>\n",
       "      <td>0.782056</td>\n",
       "      <td>0.969722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.118195</td>\n",
       "      <td>0.745884</td>\n",
       "      <td>0.818539</td>\n",
       "      <td>0.780524</td>\n",
       "      <td>0.969555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.115282</td>\n",
       "      <td>0.736548</td>\n",
       "      <td>0.827965</td>\n",
       "      <td>0.779586</td>\n",
       "      <td>0.970393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.118658</td>\n",
       "      <td>0.743158</td>\n",
       "      <td>0.831893</td>\n",
       "      <td>0.785026</td>\n",
       "      <td>0.969770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-200)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-400)... Done. 3.8s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-600)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-800)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-1000)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-1200)... Done. 3.7s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-1400)... Done. 3.8s\n",
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-1600)... Done. 3.9s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./checkpoints/mdeberta-v3-base--token-level/checkpoint-1715)... Done. 4.0s\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1715, training_loss=0.10954257211601769, metrics={'train_runtime': 355.2817, 'train_samples_per_second': 154.525, 'train_steps_per_second': 4.827, 'total_flos': 2759224000595616.0, 'train_loss': 0.10954257211601769, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(**config.wandb_init_args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_MODEL = f'checkpoints/{config.wandb_init_args[\"name\"]}/checkpoint-1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer._load_from_checkpoint(FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7833958571195564,\n",
       " 'recall': 0.8252577319587628,\n",
       " 'f1': 0.8037821102836582,\n",
       " 'accuracy': 0.9771072498502097}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = trainer.predict(dataset.test)\n",
    "test_metrics = trainer.compute_metrics((test_preds.predictions, test_preds.label_ids))\n",
    "\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10931355,
     "sourceId": 89664,
     "sourceType": "competition"
    },
    {
     "datasetId": 6604871,
     "sourceId": 10664686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
